{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vX3wcdABHWts"
   },
   "source": [
    "# 실습 3. 다음 뉴스 크롤링\n",
    "### 3-1 기사 한 페이지 크롤링 해오기  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "react_url = 'https://action.daum.net/apis/v1/reactions/home?itemKey=20240811221440047'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [추가 실습] 라프텔 리뷰 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "result = []\n",
    "\n",
    "response = requests.get('https://v.daum.net/v/20240811221440047')\n",
    "#print(response)   # 접속 권한 확인하는 상태코드\n",
    "#print(response.text)    # 데이터 포맷 확인 -> 해당 url은 html인 것을 알 수 있음\n",
    "\n",
    "# html 파싱\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# title, body 크롤링\n",
    "title = soup.select_one('h3.tit_view').text.strip()\n",
    "body = soup.select('div.article_view section p')\n",
    "body = ' '.join(p_tag.text.strip() for p_tag in body)\n",
    "\n",
    "result.append(dict(\n",
    "    title=title,\n",
    "    body=body,\n",
    "    reactions = {\n",
    "        'RECOMEND':0,\n",
    "        'LIKE' : 0,\n",
    "        'IMPRESS' : 0,\n",
    "        'ANGRY' : 0,\n",
    "        'SAD' : 0\n",
    "    }\n",
    "))\n",
    "\n",
    "headers = {\n",
    "    'Authorization':'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJqdGkiOiIzMTU4ZWRmNS1kYjYxLTQ1MjktYTQxNC1hYmFjYThiY2RkMjIiLCJjbGllbnRfaWQiOiIyNkJYQXZLbnk1V0Y1WjA5bHI1azc3WTgiLCJmb3J1bV9rZXkiOiJuZXdzIiwiZm9ydW1faWQiOi05OSwiZ3JhbnRfdHlwZSI6ImFsZXhfY3JlZGVudGlhbHMiLCJhdXRob3JpdGllcyI6WyJST0xFX0NMSUVOVCJdLCJzY29wZSI6W10sImV4cCI6MTczNDUyNTU5Nn0.vq7JLeLdZ8klf_yTOvaEDte_chYQGKt1qdoRqhGEeq0',\n",
    "    'authority':'action.daum.net',\n",
    "    'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',\n",
    "    }\n",
    "\n",
    "\n",
    "react_url = 'https://action.daum.net/apis/v1/reactions/home?itemKey=20240811221440047'\n",
    "react_res = requests.get(react_url, headers = headers)\n",
    "print(react_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': None, 'item': {'stats': {'LIKE': 0, 'DISLIKE': 0, 'GREAT': 0, 'SAD': 0, 'ABSURD': 0, 'ANGRY': 3, 'RECOMMEND': 1, 'IMPRESS': 1, 'OPTION_A': 0, 'OPTION_B': 0}, 'forumKey': 'news'}}\n",
      "RECOMMEND\n",
      "LIKE\n",
      "IMPRESS\n",
      "ANGRY\n",
      "SAD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': \"[단독] '침출수 줄줄' 여수산단 대체녹지, 오염된 토사로 조성됐다\",\n",
       "  'body': '【 앵커멘트 】여수산단 대체녹지에서 1년 넘게 발암물질이 섞인 침출수가 흘러나오고 있다는 소식, 얼마 전 전해드렸는데요. 토양오염의 원인을 찾지 못했다며 녹지를 조성한 기업도, 감독해야 할 행정기관도 손을 놓고 있습니다. 그런데 대체녹지 조성 당시, 오염된 토사가 사용됐을 가능성이 큰 것으로 확인됐습니다. 박성호 기자의 보도입니다. 【 기자 】여수산단 대체녹지에서 오염된 침출수가 처음 확인된 것은 지난해 7월. 1년 넘게 문제가 계속되고 있지만 녹지를 조성한 기업들도, 기부채납 받은 여수시도 책임을 미룬 채 지켜만 보고 있습니다. 이런 와중에 대체 녹지를 조성한 토사 가운데 절반 이상을, 건축폐기물 4천8백톤이 불법매립됐던 곳에서 가져온 사실이 새롭게 드러났습니다. KBC가 단독 입수한 대체녹지 1구역 조성 토사반입 내역 서류입니다. 전체 토사 5만㎥ 중 3만㎥, 전체 57%의 토사가 한 곳에서 반입됐는데, 이 곳은 건축폐기물 불법매립이 뒤늦게 드러난 곳이었습니다. 전문가들은 건축폐기물 불법매립으로 오염된 토사가 유입됐을 경우 침출수 등 문제가 발생할 수 있다고 말합니다. ▶ 싱크 : 환경공학 전문가(음성변조)- \"폐기물에 의해서 그 토사에 그게(오염원) 농축이 되고 그 농축된 토사에 의해서 침출이 되는 거, 그거는 이제 저희가 많이 확인이 돼왔죠.\" 대체녹지 조성 당시 토사의 오염여부를 확인해야 이 같은 문제를 방지할 수 있었지만, 법적 기준에 미치지 못 해 별다른 관리가 이뤄지지 않았습니다. ▶ 인터뷰 : 박민수 / 여수시 공원과장- \"저희가 공원 녹지를 조성할 때 10만 평방미터 이상이 되면 사전에 토양 조사를 해야 되는데 여기는 6만 2천 평방미터 정도 돼서 그 토양 조사나 이런 것들은 생략하고 저희가 공원을 조성했습니다.\" 토양오염은 드러났지만 그 원인을 찾지 못했다는 이유로 1년 넘게 방치됐던 여수산단 대체녹지 문제가 해결의 실마리를 찾을 수 있을지 주목됩니다. KBC 박성호입니다. #사건사고 #전남 #여수산단 #침출수',\n",
       "  'reactions': {'RECOMMEND': 1,\n",
       "   'LIKE': 0,\n",
       "   'IMPRESS': 1,\n",
       "   'ANGRY': 3,\n",
       "   'SAD': 0}}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "result = []\n",
    "\n",
    "response = requests.get('https://v.daum.net/v/20240811221440047')\n",
    "#print(response)   # 접속 권한 확인하는 상태코드\n",
    "#print(response.text)    # 데이터 포맷 확인 -> 해당 url은 html인 것을 알 수 있음\n",
    "\n",
    "# html 파싱\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# title, body 크롤링\n",
    "title = soup.select_one('h3.tit_view').text.strip()\n",
    "body = soup.select('div.article_view section p')\n",
    "body = ' '.join(p_tag.text.strip() for p_tag in body)\n",
    "\n",
    "result.append(dict(\n",
    "    title=title,\n",
    "    body=body,\n",
    "    reactions = {\n",
    "        'RECOMMEND':0,\n",
    "        'LIKE' : 0,\n",
    "        'IMPRESS' : 0,\n",
    "        'ANGRY' : 0,\n",
    "        'SAD' : 0\n",
    "    }\n",
    "))\n",
    "\n",
    "headers = {\n",
    "    'Authorization':'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJqdGkiOiIzMTU4ZWRmNS1kYjYxLTQ1MjktYTQxNC1hYmFjYThiY2RkMjIiLCJjbGllbnRfaWQiOiIyNkJYQXZLbnk1V0Y1WjA5bHI1azc3WTgiLCJmb3J1bV9rZXkiOiJuZXdzIiwiZm9ydW1faWQiOi05OSwiZ3JhbnRfdHlwZSI6ImFsZXhfY3JlZGVudGlhbHMiLCJhdXRob3JpdGllcyI6WyJST0xFX0NMSUVOVCJdLCJzY29wZSI6W10sImV4cCI6MTczNDUyNTU5Nn0.vq7JLeLdZ8klf_yTOvaEDte_chYQGKt1qdoRqhGEeq0',\n",
    "    'authority':'action.daum.net',\n",
    "    'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',\n",
    "    }\n",
    "\n",
    "\n",
    "react_url = 'https://action.daum.net/apis/v1/reactions/home?itemKey=20240811221440047'\n",
    "react_res = requests.get(react_url, headers = headers)\n",
    "react_data = react_res.json()\n",
    "print(react_data)\n",
    "\n",
    "for react in result[0]['reactions']:\n",
    "    print(react)\n",
    "    result[0]['reactions'][react] = react_data['item']['stats'][react]\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sesac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
